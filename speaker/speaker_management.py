#
# Speaker management implementation
# author:   L. De Marchis
# updated:  18/10/2021
# based on code from https://github.com/philipperemy/deep-speaker
#
import json
import os
import numpy as np
from json import JSONEncoder

from pydub import AudioSegment

# our new utilities
from audio_utils_new import *

from utilities import compute_embeddings, create_path, read_wav, NumpyArrayEncoder

# global settings here
import config

from constants import SAMPLE_RATE, NUM_FRAMES
 
# define the duration of the segment you want to use in ms
SEGMENT_DURATION = 1000
# define by how much you want the segments to overlap
SEGMENT_STEP = int(SEGMENT_DURATION/3)
 
# encoder used for saving the dictionary in a json format
# imported from utilities
 
# function for computing the centroid
# modified: it is important to work on a copy
def calculate_centroid(ds, label, centroids):
    # centroid is the old structure with all centroids

    # get all samples embedding vectors of the speaker
    samples = [ds[key] for key in ds.keys()]
 
    # compute centroid
    centroid = np.mean(samples, axis=0)

    # operate on a copy, to avoid corrupt the current in memory
    new_centroids = centroids.copy()

    # save the key in data structure with norm equal to one
    new_centroids[label] = centroid/np.linalg.norm(centroid)
    
    return new_centroids

#
# modified, does not use directly the model, but compute_embeddings from 
# utilities
#
def add_file_to_dict(file, ds, key, model):
    ds[key] = compute_embeddings(file, model)
    
    return ds

#
#
# this function add a new speaker to the centroids data structure
# and returns the new data structure
#
def add_speaker(speaker_name, filename, old_centroids, model):

    # model is the Keras model

    # L.S.: I have removed the check on the file extension. It is not applicable in the context
    # of the REST service (we get a stream of bytes)
    # Import wav file
    Audio = AudioSegment.from_wav(filename)
    # Resample to 16000 Hz (L.S.: ??)
    Audio = Audio.set_frame_rate(SAMPLE_RATE)        
 
    # initialize the new-speaker temp data structure
    new_data_structure = {}
 
    # generate several segments 
    # move by a quarter of second, thus the wav files will have some overlapping part, 
    # but more files can be generated by a shorter speech
    for i in range(0, len(Audio), SEGMENT_STEP):
        print("processing audio segment", int(i/SEGMENT_STEP))

        # each index refers to a millisecond, hence [0:1000] is a second     
        segment = Audio[i:i+SEGMENT_DURATION] 
        
        # the name for the temp segment file, better to add the path.. it is done in read_wav
        F_NAME = '%d.wav'%(i)
        F_PATH = create_path(F_NAME)

        #Exports to a wav file in the current path.
        segment.export(F_PATH, format="wav") 
 
        # Check that the last segment of the sample is greater than one second
        if len(Audio)-i >= SEGMENT_DURATION:
            # reload the file
            bytes_read = read_wav(F_NAME)

            # Add the embedding vector to the temp data structure
            new_data_structure = add_file_to_dict(bytes_read, new_data_structure, i, model)
 
        #remove the file once used
        os.remove(F_NAME)
 
    # Update the centroids data structure    
    new_centroids = calculate_centroid(new_data_structure, speaker_name, old_centroids)
 
    # Save the updated data structure in a new json file
    # with open(config.global_settings['NEW_CENTROIDS_FILE_NAME'], 'w') as fp:
    #    json.dump(new_centroids, fp, cls=NumpyArrayEncoder)
 
    return new_centroids